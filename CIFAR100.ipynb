{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WomsZbsTrMcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5cxqVTQ_Jtl",
        "outputId": "b3da6c5e-4f58-4652-dee7-5f9099c3afd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dadaptation\n",
            "  Downloading dadaptation-3.2.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: dadaptation\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dadaptation: filename=dadaptation-3.2-py3-none-any.whl size=23208 sha256=97a1463f46c17a7148184df4930daf5b4d9c06d66dc7264391b56039a415db15\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/03/6d/feba04df15ef39d9ac4e3504058ac2a88fb2ef9183ba92b111\n",
            "Successfully built dadaptation\n",
            "Installing collected packages: dadaptation\n",
            "Successfully installed dadaptation-3.2\n"
          ]
        }
      ],
      "source": [
        "pip install dadaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F0nd6MFvApSc"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dadaptation.dadapt_adagrad import DAdaptAdaGrad\n",
        "from dadaptation.dadapt_adam import DAdaptAdam\n",
        "from dadaptation.dadapt_sgd import DAdaptSGD\n",
        "from dadaptation.dadapt_adan import DAdaptAdan\n",
        "from dadaptation.dadapt_lion import DAdaptLion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUhJpQtWhZbE",
        "outputId": "941ab566-4d89-423a-d6f4-5f01f3f78789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:18<00:00, 9132710.20it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the Wide Residual Block\n",
        "class WideResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, widen_factor):\n",
        "        super(WideResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels * widen_factor, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels * widen_factor)\n",
        "        self.conv2 = nn.Conv2d(out_channels * widen_factor, out_channels * widen_factor, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels * widen_factor)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * widen_factor:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * widen_factor, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * widen_factor)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the Wide Residual Network (WRN)\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, padding=1, bias=False)  # 3 channels for CIFAR-100\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "\n",
        "        n = (depth - 4) // 6\n",
        "        self.layer1 = self._make_layer(WideResNetBlock, 16, n, stride=1, widen_factor=widen_factor)\n",
        "        self.layer2 = self._make_layer(WideResNetBlock, 32, n, stride=2, widen_factor=widen_factor)\n",
        "        self.layer3 = self._make_layer(WideResNetBlock, 64, n, stride=2, widen_factor=widen_factor)\n",
        "\n",
        "        self.fc = nn.Linear(64 * widen_factor, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, n, stride, widen_factor):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, widen_factor))\n",
        "        self.in_channels = out_channels * widen_factor\n",
        "        for _ in range(1, n):\n",
        "            layers.append(block(self.in_channels, out_channels, 1, widen_factor))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Prepare the CIFAR-100 dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, and optimizers\n",
        "model = WideResNet(depth=16, widen_factor=8, num_classes=100).to(device)  # WRN-16-8 for CIFAR-100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_dadapt = DAdaptAdam(model.parameters())  # Ensure DAdaptAdam is defined or imported\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Define a training function\n",
        "def train(model, optimizer, epochs, train_loader, test_loader, criterion):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    test_accs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Training loop\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Evaluate test accuracy after each epoch\n",
        "        test_acc = test(model, test_loader)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "    return train_losses, test_accs\n",
        "\n",
        "# Define a testing function to evaluate model on the test set\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Train with DAdaptAdam\n",
        "epochs = 100\n",
        "train_losses_dadapt, test_accs_dadapt = train(model, optimizer_dadapt, epochs, train_loader, test_loader, criterion)\n",
        "\n",
        "# Train with SGD\n",
        "model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "train_losses_sgd, test_accs_sgd = train(model, optimizer_sgd, epochs, train_loader, test_loader, criterion)\n",
        "\n",
        "# Plotting Loss and Accuracy Comparison\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Loss Comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses_dadapt, label='DAdaptAdam', color='blue')\n",
        "plt.plot(train_losses_sgd, label='SGD', color='orange')\n",
        "plt.title('Training Loss Comparison (CIFAR-100)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy Comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_accs_dadapt, label='DAdaptAdam', color='blue')\n",
        "plt.plot(test_accs_sgd, label='SGD', color='orange')\n",
        "plt.title('Test Accuracy Comparison (CIFAR-100)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4COSCYWrUyrK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}